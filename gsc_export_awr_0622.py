# -*- coding: utf-8 -*-
"""GSC_Export_AWR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PO7idxO_EPkJH7zB75R4wPt7No1KE4Lc
"""

import requests
import json
import pandas as pd
from pandas import json_normalize
from google.colab import files
from io import BytesIO
from urllib.request import urlopen
from zipfile import ZipFile
import requests, json, glob, os, csv, time
from datetime import datetime
import re

token='7752ee848bac741a1b2ccb1be43a19fd'
AV_token="0b8e6db6509f2bf80bda79ad71f5a6b6"

def get_project_id(proj_name,i=token):
    url = "https://api.awrcloud.com/api/marketshare/v1/endpoint.php?action=getProjects&token={}".format(i)
    response = requests.get(url).json()
    ID_df = json_normalize(response)
    proj_id = ID_df.iloc[list(ID_df.iloc[:,1]).index(proj_name),0]
    return proj_id
#test1 = get_project_id("LG.com_UK")

def get_search_engines_id(proj_id,mytoken=token):
    url = "https://api.awrcloud.com/api/marketshare/v1/endpoint.php?action=getSearchEngines&token={}&projectId={}".format(mytoken,proj_id)
    response = requests.get(url).json()
    se_df = json_normalize(response)
    id = list(se_df['id']).pop()
    return id

def get_search_engines_name(proj_id,mytoken=token):
    url = "https://api.awrcloud.com/api/marketshare/v1/endpoint.php?action=getSearchEngines&token={}&projectId={}".format(mytoken,proj_id)
    response = requests.get(url).json()
    se_df = json_normalize(response)
    id = list(se_df['name']).pop()
    return id

def get_keyword_group_df(proj_id=111, i=token):
    url = "https://api.awrcloud.com/api/marketshare/v1/endpoint.php?action=getKeywordGroups&token={}&projectId={}".format(i,proj_id)
    response = requests.get(url).json()
    group_df = json_normalize(response)
    return group_df
#test2 = getkeyword_group_df(test1)

def get_keyword_group_id(group_name, proj_id=111, i=token):
    url = "https://api.awrcloud.com/api/marketshare/v1/endpoint.php?action=getKeywordGroups&token={}&projectId={}".format(i,proj_id)
    response = requests.get(url).json()
    group_df = json_normalize(response)
    group_id = group_df.iloc[list(group_df.iloc[:,1]).index(group_name),0]
    return group_id
#test3 = get_keyword_group_id('Brand',test1)

def get_metadata_df(proj_id,group_id,obj_date,i=token):
    meta_df = pd.DataFrame({})
    url = "https://api.awrcloud.com/api/marketshare/v1/endpoint.php?action=getMetadata&token={}&projectId={}&searchEngineId={}&keywordGroupId={}&startDate={}&stopDate={}&mode=plain".format(i,proj_id,-1,group_id,obj_date,obj_date)
    print(group_id)
    response = requests.get(url).text
    domain_list_raw = response.split('<br/>')
    colum_name = domain_list_raw[0].split(',')
    columindex=0
    for x in colum_name:
        flashdata= []
        for i in domain_list_raw[1:-1]:
            flashdata.append(i.split(',')[columindex])
        meta_df[x]=flashdata
        columindex = columindex+1
    return meta_df
#test4 = get_metadata_df(proj_id='111',group_id='33',obj_date='2021-03-29',i=token)
#test4 = get_metadata_df(test1,test3,'2021-03-30')

def get_domain_urls(proj_id,group_id,obj_date,obj_domain,i=token):
    url = "https://api.awrcloud.com/api/marketshare/v1/endpoint.php?action=getDomainUrls&token={}&projectId={}&searchEngineId=-1&keywordGroupId={}&date={}&domain={}&mode=plain".format(i,proj_id,group_id,obj_date,obj_domain)
    response = requests.get(url).text
    url_list_raw = response.split('<br/>')
    colum_name = url_list_raw[0].split(',')
    url_index=colum_name.index('url')
    colum_count=len(colum_name)
    url_df = pd.DataFrame(columns = colum_name)
    for i in url_list_raw[1:-1]:
        flashdata= []
        row_list = i.split(',')
        flashdata = flashdata+row_list[:url_index]
        flashdata.append(','.join(row_list[url_index:url_index-colum_count+1]))
        flashdata = flashdata+row_list[url_index-colum_count+1:]
        url_df = url_df.append(pd.Series(flashdata,index=url_df.columns), ignore_index=True)
    url_df=url_df.astype({'market share':'float','estimated visits':'float','click share':'float'})
    return url_df
#test5 = get_domain_urls(proj_id='111',group_id='33',obj_date='2021-03-29',obj_domain='lg.com',i=token)


def export_top_sites_old(av_proj_name,start,stop,searchE, num_url=12,token = AV_token):
    proj_name = '+'.join(av_proj_name.split())
    url = "https://api.awrcloud.com/v2/get.php?action=topsites_export&project={}&token={}&startDate={}&stopDate={}&pixelPosition=false&topUrls={}&searchEngine={}".format(proj_name, token, start, stop,num_url,searchE)
    response1 = requests.get(url).json()
    res_df = json_normalize(response1)
    down_url = res_df['details'][0]
    return down_url

 # 상위 노출 컨텐츠 추출 함수_키워드 추출
def export_top_sites_group(proj_name,  start_date, end_date, kwg_id, num_url=12, token=AV_token):
    url = "https://api.awrcloud.com/v2/get.php?action=topsites_export&project={}&token={}&startDate={}&stopDate={}&keywordGroupId={}&pixelPosition=true&sponsored=False&topUrls={}&xlsx=true".format(proj_name, token, start_date, end_date, str(kwg_id), num_url) 
    response = requests.get(url).json()
    return response


 # 상위 노출 컨텐츠 추출 함수
def export_top_sites(proj_name, start_date, end_date, num_url=12, token=AV_token): 
    url = "https://api.awrcloud.com/v2/get.php?action=topsites_export&project={}&token={}&startDate={}&stopDate={}&pixelPosition=true&sponsored=False&topUrls={}&xlsx=true".format(proj_name, token, start_date, end_date, num_url)
    response = requests.get(url).json()
    return response

 # 상위 노출 컨텐츠 추출 함수_키워드 추출
def export_gsc_data(proj_name,start_date, end_date, token=token):
    url = "https://api.awrcloud.com/v2/get.php?action=gwt_export&token={}&project={}&device=1&startDate={}&stopDate={}".format(token,proj_name, start_date, end_date) 
    response = requests.get(url).json()
    res_df = json_normalize(response)
    gsc_zip_url = res_df['details'][0]
    return gsc_zip_url

def date_range(start_date, end_date):
    start = datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.strptime(end_date, "%Y-%m-%d")
    dates = [date.strftime("%Y-%m-%d") for date in pd.date_range(start, periods=(end-start).days+1)]
    return dates

def zip_url_open(url,path):
    status = ""
    with requests.get(url) as zipresp:
        time.sleep(3)
        try:
            with ZipFile(BytesIO(zipresp.content)) as zfile:
                time.sleep(2)
                zfile.extractall(path)
                print(x+" "+y+" done")
                status = "Good"
        except: 
            print("zip file extract fail")
            status = "Bad"
            pass
    return status

################# < 입 력 부 > ###################
 # 날짜
start_date = "2021-05-14"   
end_date = "2021-06-13"   
date_list = date_range(start_date,end_date)
 # 프로젝트
project_type = "LG.com"  
proj_list = ['Spain','Russia','Turkey'] 
keyword_set_df=pd.DataFrame({})
keyword_set_df['LG.com_Spain'] = ["lg oled","lg oled 55","lg oled 65","oled lg","lg 65 oled","oled48c14lb","oled55g16la","oled65g16la","oled65c16la","oled48c16la"]
keyword_set_df['LG.com_Russia'] = ["lg oled","lg oled 55","телевизор lg oled","lg oled 65","oled lg","lg c1","lg c1 oled","lg g1","oled48c1rla","lg oled55c1rla"]
keyword_set_df['LG.com_Turkey'] = ["lg oled tv","lg oled evo","lg oled","lg oled 55","lg oled 65","lg c1 oled","lg c1","lg g1","lg g1 oled","lg c1 48"]

 # 경로
path = "/content/sample_data/"
os.chdir(path)
print(os.getcwd())
#################################################

## Project list에 type 붙여주기
for i in range(len(proj_list)):
    proj_list[i] = project_type+"_"+proj_list[i]
print(proj_list)

extract_fail_urls = []

## zipurl 받고 풀고 csv 열기
for x in proj_list:
    select_name = x
    proj_name = '+'.join(select_name.split())
    proj_id = get_project_id(select_name) 
    ## export GSC url 저장 및
    url_list=[]
    new_path = path+x
    try:
        os.mkdir(new_path)
    except:
        pass
    os.chdir(new_path)
    for y in date_list:
        zipurl = export_gsc_data(proj_name, y, y)
        url_list.append(zipurl)   # 파일 다운로드 url을 list에 저장
        file_name = zipurl.split('fileName=')[1]
        stat = zip_url_open(zipurl,os.getcwd())
        interv = 0
        while stat == "Bad":
            stat = zip_url_open(zipurl,os.getcwd())
            interv += 1
            if interv > 4:
                extract_fail_urls.append(zipurl)
                break
            else:
                pass

    GSC_df = pd.DataFrame({})
    obj_GSC_df = pd.DataFrame({})
    file_list = glob.glob(new_path+'/*.csv')
    file_list = sorted(file_list)

    for z in file_list:
        print(z)
        flash_df = pd.read_csv(z)
        time.sleep(1)
        regex = re.compile(r"\d+-\d+-\d+")

        obj_key_set_df = pd.DataFrame({})
        obj_key_set_df['Dates'] = [regex.search(z).group() for x in range(10)]
        obj_key_set_df['Group'] = ['LG OLED','LG OLED','LG OLED','LG OLED','LG OLED','LG OLED Model (2021)','LG OLED Model (2021)','LG OLED Model (2021)','LG OLED Model (2021)','LG OLED Model (2021)']
        obj_key_set_df['Keyword'] = keyword_set_df[x]
        
        flash_df['Date'] = [regex.search(z).group() for x in range(len(flash_df))]
        flash_df["Clicks"] = flash_df["Clicks"].apply(lambda r: 0 if r=="-" else int(r))
        flash_df['CTR_calculated'] = flash_df.Clicks / flash_df.Impressions*100
        flash_df["Clicks"] = flash_df["Clicks"].apply(lambda r: "-" if r== 0 else r)
        flash_df['CTR_calculated']= flash_df["CTR_calculated"].apply(lambda r: "-" if r== 0 else r)
        flash_df = flash_df[['Date','Keyword','Impressions','Clicks','CTR_calculated','Avg Position','Clicks Opportunities']]

        obj_key_set_df.set_index('Keyword',inplace=True)
        join_flash = flash_df.set_index('Keyword',inplace=False)

        obj_GSC_data = obj_key_set_df.join(join_flash,on='Keyword', how='left')
        obj_GSC_data = obj_GSC_data.drop('Date',axis=1)
        obj_GSC_data = obj_GSC_data.fillna("-")
        obj_GSC_data=obj_GSC_data.reset_index()
        obj_GSC_data=obj_GSC_data[['Dates','Group','Keyword','Impressions','Clicks','CTR_calculated','Avg Position','Clicks Opportunities']]

        GSC_df = pd.concat([GSC_df,flash_df])
        GSC_df.reset_index(drop=True)
        obj_GSC_df =  pd.concat([obj_GSC_df,obj_GSC_data])
        obj_GSC_df.reset_index(drop=True)


    with pd.ExcelWriter("{}_GSC_{}_{}.xlsx".format(x,start_date,end_date)) as writer:
        GSC_df.to_excel(writer,sheet_name="raw")
        obj_GSC_df.to_excel(writer,sheet_name="result")

    files.download("{}_GSC_{}_{}.xlsx".format(x,start_date,end_date))
    time.sleep(5)
    print(extract_fail_urls)