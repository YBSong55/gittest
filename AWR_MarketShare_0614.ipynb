{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AWR MarketShare 0614",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBDAkPVQifOl"
      },
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from pandas import json_normalize\n",
        "from google.colab import files\n",
        "\n",
        "token='7752ee848bac741a1b2ccb1be43a19fd'\n",
        "AV_token=\"0b8e6db6509f2bf80bda79ad71f5a6b6\"\n",
        "\n",
        "def get_project_id(proj_name,i=token):\n",
        "    url = \"https://api.awrcloud.com/api/marketshare/v1/endpoint.php?action=getProjects&token={}\".format(i)\n",
        "    response = requests.get(url).json()\n",
        "    ID_df = json_normalize(response)\n",
        "    proj_id = ID_df.iloc[list(ID_df.iloc[:,1]).index(proj_name),0]\n",
        "    return proj_id\n",
        "#test1 = get_project_id(\"LG.com_UK\")\n",
        "\n",
        "def get_search_engines_id(proj_id,mytoken=token):\n",
        "    url = \"https://api.awrcloud.com/api/marketshare/v1/endpoint.php?action=getSearchEngines&token={}&projectId={}\".format(mytoken,proj_id)\n",
        "    response = requests.get(url).json()\n",
        "    se_df = json_normalize(response)\n",
        "    id = list(se_df['id']).pop()\n",
        "    return id\n",
        "\n",
        "def get_search_engines_name(proj_id,mytoken=token):\n",
        "    url = \"https://api.awrcloud.com/api/marketshare/v1/endpoint.php?action=getSearchEngines&token={}&projectId={}\".format(mytoken,proj_id)\n",
        "    response = requests.get(url).json()\n",
        "    se_df = json_normalize(response)\n",
        "    id = list(se_df['name']).pop()\n",
        "    return id\n",
        "\n",
        "def get_keyword_group_df(proj_id=111, i=token):\n",
        "    url = \"https://api.awrcloud.com/api/marketshare/v1/endpoint.php?action=getKeywordGroups&token={}&projectId={}\".format(i,proj_id)\n",
        "    response = requests.get(url).json()\n",
        "    group_df = json_normalize(response)\n",
        "    return group_df\n",
        "#test2 = getkeyword_group_df(test1)\n",
        "\n",
        "def get_keyword_group_id(group_name, proj_id=111, i=token):\n",
        "    url = \"https://api.awrcloud.com/api/marketshare/v1/endpoint.php?action=getKeywordGroups&token={}&projectId={}\".format(i,proj_id)\n",
        "    response = requests.get(url).json()\n",
        "    group_df = json_normalize(response)\n",
        "    group_id = group_df.iloc[list(group_df.iloc[:,1]).index(group_name),0]\n",
        "    return group_id\n",
        "#test3 = get_keyword_group_id('Brand',test1)\n",
        "\n",
        "def get_metadata_df(proj_id,group_id,obj_date,i=token):\n",
        "    meta_df = pd.DataFrame({})\n",
        "    url = \"https://api.awrcloud.com/api/marketshare/v1/endpoint.php?action=getMetadata&token={}&projectId={}&searchEngineId={}&keywordGroupId={}&startDate={}&stopDate={}&mode=plain\".format(i,proj_id,-1,group_id,obj_date,obj_date)\n",
        "    print(group_id)\n",
        "    response = requests.get(url).text\n",
        "    domain_list_raw = response.split('<br/>')\n",
        "    colum_name = domain_list_raw[0].split(',')\n",
        "    columindex=0\n",
        "    for x in colum_name:\n",
        "        flashdata= []\n",
        "        for i in domain_list_raw[1:-1]:\n",
        "            flashdata.append(i.split(',')[columindex])\n",
        "        meta_df[x]=flashdata\n",
        "        columindex = columindex+1\n",
        "    return meta_df\n",
        "#test4 = get_metadata_df(proj_id='111',group_id='33',obj_date='2021-03-29',i=token)\n",
        "#test4 = get_metadata_df(test1,test3,'2021-03-30')\n",
        "\n",
        "def get_domain_urls(proj_id,group_id,obj_date,obj_domain,i=token):\n",
        "    url = \"https://api.awrcloud.com/api/marketshare/v1/endpoint.php?action=getDomainUrls&token={}&projectId={}&searchEngineId=-1&keywordGroupId={}&date={}&domain={}&mode=plain\".format(i,proj_id,group_id,obj_date,obj_domain)\n",
        "    response = requests.get(url).text\n",
        "    url_list_raw = response.split('<br/>')\n",
        "    colum_name = url_list_raw[0].split(',')\n",
        "    url_index=colum_name.index('url')\n",
        "    colum_count=len(colum_name)\n",
        "    url_df = pd.DataFrame(columns = colum_name)\n",
        "    for i in url_list_raw[1:-1]:\n",
        "        flashdata= []\n",
        "        row_list = i.split(',')\n",
        "        flashdata = flashdata+row_list[:url_index]\n",
        "        flashdata.append(','.join(row_list[url_index:url_index-colum_count+1]))\n",
        "        flashdata = flashdata+row_list[url_index-colum_count+1:]\n",
        "        url_df = url_df.append(pd.Series(flashdata,index=url_df.columns), ignore_index=True)\n",
        "    url_df=url_df.astype({'market share':'float','estimated visits':'float','click share':'float'})\n",
        "    return url_df\n",
        "#test5 = get_domain_urls(proj_id='111',group_id='33',obj_date='2021-03-29',obj_domain='lg.com',i=token)\n",
        "\n",
        "\n",
        "def export_top_sites(av_proj_name,start,stop,searchE, num_url=12,token = AV_token):\n",
        "    proj_name = '+'.join(av_proj_name.split())\n",
        "    url = \"https://api.awrcloud.com/v2/get.php?action=topsites_export&project={}&token={}&startDate={}&stopDate={}&pixelPosition=false&topUrls={}&searchEngine={}\".format(proj_name, token, start, stop,num_url,searchE)\n",
        "    response1 = requests.get(url).json()\n",
        "    res_df = json_normalize(response1)\n",
        "    down_url = res_df['details'][0]\n",
        "    return down_url\n",
        "\n",
        " # 상위 노출 컨텐츠 추출 함수_키워드 추출\n",
        "def export_top_sites_group(proj_name,  start_date, end_date, kwg_id, num_url=12, token=AV_token):\n",
        "    url = \"https://api.awrcloud.com/v2/get.php?action=topsites_export&project={}&token={}&startDate={}&stopDate={}&keywordGroupId={}&pixelPosition=true&sponsored=False&topUrls={}&xlsx=true\".format(proj_name, token, start_date, end_date, str(kwg_id), num_url) \n",
        "    response = requests.get(url).json()\n",
        "    return response\n",
        "\n",
        "\n",
        " # 상위 노출 컨텐츠 추출 함수\n",
        "def export_top_sites(proj_name, start_date, end_date, num_url=12, token=AV_token): \n",
        "    url = \"https://api.awrcloud.com/v2/get.php?action=topsites_export&project={}&token={}&startDate={}&stopDate={}&pixelPosition=true&sponsored=False&topUrls={}&xlsx=true\".format(proj_name, token, start_date, end_date, num_url)\n",
        "    response = requests.get(url).json()\n",
        "    return response\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF4z0NzuT9Xa"
      },
      "source": [
        "# # 입력부\n",
        "insert_name = \"LG.com_Spain\"\n",
        "select_date = \"2021-06-13\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7x4_ZprTxtw"
      },
      "source": [
        "# Market Share 키워드 그룹별 세부 url 리스트 추출(시트별) \n",
        "proj_id_re = get_project_id(insert_name)\n",
        "group_name = get_keyword_group_df(proj_id_re)['name']\n",
        "group_id = get_keyword_group_df(proj_id_re)['id']\n",
        "filename = \"{}_MarketShare_{}\".format(insert_name,select_date)\n",
        "count = 1\n",
        "with pd.ExcelWriter('{}.xlsx'.format(filename)) as writer:\n",
        "    for x in group_id[1:]:\n",
        "        all_df = pd.DataFrame({})\n",
        "        sub_df= get_metadata_df(proj_id = proj_id_re,group_id = x, obj_date = select_date,i=token)\n",
        "        # all_df = pd.concat([all_df,sub_df.iloc[:,5:]])\n",
        "        b = sub_df['domain']\n",
        "        for y in b:\n",
        "            try:\n",
        "                sub_df2 = get_domain_urls(proj_id_re,x,select_date,y,token)\n",
        "                all_df = pd.concat([all_df,sub_df2.iloc[:,5:]])\n",
        "                print(\"{} 도메인\".format(y))\n",
        "            except:\n",
        "                exception_flash = [y for ii in range(len(all_df.columns))]\n",
        "                all_df = all_df.append(pd.Series(exception_flash,index=all_df.columns), ignore_index=True)\n",
        "        all_df.to_excel(writer,sheet_name=group_name[count],index=True)\n",
        "        count=count+1\n",
        "files.download('{}.xlsx'.format(filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOsS125z8pDa"
      },
      "source": [
        "# AWR 프로젝트 별 등록 키워드셋 리스트 추출(시트별) \n",
        "filename = \"AWR_keywordset_{}\".format(select_date)\n",
        "url = \"https://api.awrcloud.com/v2/get.php?action=projects&token={}\".format(token)\n",
        "response = requests.get(url).json()\n",
        "pj_df = json_normalize(response['projects'])\n",
        "is_LGcom_pj = pj_df['name'].str.contains(\"LG.com_\")\n",
        "is_daily_pj = pj_df['frequency'] == 'daily'\n",
        "obj_pj_df = pj_df[is_LGcom_pj & is_daily_pj]\n",
        "group_1_list = ['Generic','Core','Brand','Samsung Core','Samsung Brand']\n",
        "with pd.ExcelWriter('{}.xlsx'.format(filename)) as writer:\n",
        "    obj_pj_df.to_excel(writer,sheet_name=\"project_list\",index=True)\n",
        "    for y in obj_pj_df['name']:\n",
        "        print(y)\n",
        "        url = \"https://api.awrcloud.com/v2/get.php?action=details&project={}&token={}\".format(y,token)\n",
        "        response2 = requests.get(url).json()\n",
        "        a = response2.keys()\n",
        "        kw_detail_df = json_normalize(response2['keywords'])\n",
        "        # try:\n",
        "        dicc_v1 = [','.join(list(set(x)&set(group_1_list))) for x in kw_detail_df['kw_groups']]\n",
        "        dicc_v2 = [','.join(list(set(x) - set(group_1_list))) for x in kw_detail_df['kw_groups']]\n",
        "        dicc_k = [y for y in kw_detail_df['name']]\n",
        "        # except:\n",
        "        #     dicc_v1 =['fail','fail']\n",
        "        #     dicc_v2=['fail','fail']\n",
        "        #     dicc_k=['fail','fail']\n",
        "        kwdicc_df = pd.DataFrame({'group_1':dicc_v1,'group_2':dicc_v2},index=dicc_k,dtype=str)\n",
        "        kwdicc_df.to_excel(writer,sheet_name= y,index=True)\n",
        "files.download('{}.xlsx'.format(filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x119fQ9bXwe"
      },
      "source": [
        "# AWR 프로젝트 별 등록 컴피티터 리스트 추출(시트별) \n",
        "filename = \"AWR_competitors_{}\".format(select_date)\n",
        "url = \"https://api.awrcloud.com/v2/get.php?action=projects&token={}\".format(token)\n",
        "response = requests.get(url).json()\n",
        "pj_df = json_normalize(response['projects'])\n",
        "is_LGcom_pj = pj_df['name'].str.contains(\"LG.com_\")\n",
        "is_daily_pj = pj_df['frequency'] == 'daily'\n",
        "obj_pj_df = pj_df[is_LGcom_pj & is_daily_pj]\n",
        "group_1_list = ['Generic','Core','Brand','Samsung Core','Samsung Brand']\n",
        "with pd.ExcelWriter('{}.xlsx'.format(filename)) as writer:\n",
        "    obj_pj_df.to_excel(writer,sheet_name=\"project_list\",index=True)\n",
        "    for y in obj_pj_df['name']:\n",
        "        url = \"https://api.awrcloud.com/v2/get.php?action=details&project={}&token={}\".format(y,token)\n",
        "        response2 = requests.get(url).json()\n",
        "        a = response2.keys()\n",
        "        website_df = json_normalize(response2['websites'])\n",
        "        website_df.to_excel(writer,sheet_name= y,index=True)\n",
        "files.download('{}.xlsx'.format(filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFrLlpXolRbn"
      },
      "source": [
        "# Market Share 키워드 그룹별 세부 url 개수 count (일자 번들 입력) \n",
        "date_list = pd.date_range('2021-05-24', periods=7)\n",
        "fail_df= pd.DataFrame({},columns =['f_project','f_group','f_date'])\n",
        "proj_list = []\n",
        "\n",
        "for xxx in proj_list:\n",
        "    proj_id_re = get_project_id(xxx)\n",
        "    print(proj_id_re)\n",
        "    group_df = get_keyword_group_df(proj_id_re)\n",
        "\n",
        "    # Keyword Group 대상 설정 부분\n",
        "    is_general_cate = group_df['name'] == 'General - Category'\n",
        "    # is_general_cate = group_df['name'] == 'General - Category'    \n",
        "    group_select = group_df[is_general_cate]\n",
        "\n",
        "    group_select.reset_index(inplace=True)\n",
        "    group_select\n",
        "    group_name = group_select['name']\n",
        "    group_id = group_select['id']\n",
        "    for x in group_id[:]:\n",
        "        all_df = pd.DataFrame({})\n",
        "        count = 0\n",
        "        filename = \"{}_result_{}\".format(xxx,x)\n",
        "        for date in date_list:\n",
        "            select_date = str(date)[:10]\n",
        "            print(select_date)\n",
        "            try:\n",
        "                sub_df= get_metadata_df(proj_id = proj_id_re,group_id = x, obj_date = str(select_date),i=token)\n",
        "                sub_df = sub_df.astype({'urls': 'float'})\n",
        "            except:\n",
        "                flash_data = [xxx, x, select_date]\n",
        "                fail_df = fail_df.append(pd.Series(flash_data, index=fail_df.columns), ignore_index=True)\n",
        "                print(fail_df)\n",
        "                sub_df = pd.DataFrame({})\n",
        "                pass\n",
        "            all_df = pd.concat([all_df,sub_df])\n",
        "        grouped_df = all_df['urls'].groupby(all_df['domain'])\n",
        "        aa = grouped_df.sum()\n",
        "        bb = aa.sort_values(ascending=False)\n",
        "        count=count+1\n",
        "        bb.to_excel('{}.xlsx'.format(filename),index=True)\n",
        "        print(count)\n",
        "    files.download('{}.xlsx'.format(filename))\n",
        "    print(fail_df)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}